{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "# Constants\n",
    "CSV_FILE_PATH = \"utterance.csv\"\n",
    "KEYWORDS_CSV_FILE_PATH = 'keywords_tfidf.csv'\n",
    "TOP_N_KEYWORDS = 50\n",
    "TOP_N = 5\n",
    "NUM_TESTS = 18000\n",
    "CUSTOM_STOPWORDS = {}\n",
    "ENABLE_LEMMATIZATION = True\n",
    "\n",
    "# Download the stopwords if not already downloaded\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove punctuation and stopwords\n",
    "        tokens = [word for word in text.split() if word.isalnum() and word not in stop_words]\n",
    "        # POS tagging\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        # Lemmatize tokens with POS tags\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return text\n",
    "\n",
    "# Load the keywords and utterance data\n",
    "keywords_df = pd.read_csv('keywords.csv')\n",
    "utterance_df = pd.read_csv('utterance.csv')\n",
    "\n",
    "utterance_df['utterance'] = utterance_df['utterance'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_data, test_data = train_test_split(utterance_df, test_size=0.8, random_state=41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = min(10, len(intent_data))\n",
    "display(utterance_df['utterance'].sample(n=sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(utterance_df['utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocess_text(\"i  wanna set up my shipping address\")\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=list(stop_words), ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_data['utterance'])\n",
    "y = train_data['intent']\n",
    "\n",
    "# Step 4: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Step 5: Train Model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate Model to get score predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Step 4: Keyword Extraction\n",
    "keywords_list = []\n",
    "\n",
    "for (category, intent) in train_data[['category', 'intent']].drop_duplicates().itertuples(index=False):\n",
    "    intent_data = train_data[(train_data['category'] == category) & (train_data['intent'] == intent)]\n",
    "    intent_vectorizer = TfidfVectorizer(max_features=20, stop_words=list(stop_words), ngram_range=(1, 3))\n",
    "    intent_X = intent_vectorizer.fit_transform(intent_data['utterance'])\n",
    "    feature_names = intent_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = intent_X.mean(axis=0).A1  # Average TF-IDF scores for each keyword\n",
    "\n",
    "    for keyword, score in zip(feature_names, tfidf_scores):\n",
    "        keywords_list.append({\n",
    "            'category': category,\n",
    "            'intent': intent,\n",
    "            'keyword': keyword,\n",
    "            'tfidf score': score\n",
    "        })\n",
    "\n",
    "# Step 5: Save Keywords\n",
    "keywords_df = pd.DataFrame(keywords_list)\n",
    "keywords_df.to_csv('keywords.csv', index=False)\n",
    "\n",
    "print(\"Keywords have been extracted and saved to 'keywords.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PREDICT = 1\n",
    "\n",
    "# Create a dictionary for quick keyword lookup\n",
    "keywords_dict = {}\n",
    "for _, row in keywords_df.iterrows():\n",
    "    if row['intent'] not in keywords_dict:\n",
    "        keywords_dict[row['intent']] = set()\n",
    "    keywords_dict[row['intent']].add(row['keyword'])\n",
    "\n",
    "# Function to get top N predicted intents based on keywords\n",
    "def get_top_n_intents(text, keywords_dict, n=1):\n",
    "    #text = preprocess_text(text)\n",
    "    intent_scores = {}\n",
    "    keyword_used = {}\n",
    "    for intent, keywords in keywords_dict.items():\n",
    "        score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in text:\n",
    "                score += 1\n",
    "                keyword_used[intent] = keyword\n",
    "        if score > 0:\n",
    "            intent_scores[intent] = score\n",
    "    sorted_intents = sorted(intent_scores, key=intent_scores.get, reverse=True)\n",
    "    return sorted_intents[:n], [keyword_used[intent] for intent in sorted_intents[:n]]\n",
    "\n",
    "# Predict the top N intents for the test data\n",
    "test_data['top_predict_intents'], test_data['keywords_used'] = zip(*test_data['utterance'].apply(lambda x: get_top_n_intents(x, keywords_dict, n=TOP_PREDICT)))\n",
    "\n",
    "# Check if the actual intent is in the top N predicted intents\n",
    "test_data['is_accurate'] = test_data.apply(lambda row: row['intent'] in row['top_predict_intents'], axis=1)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy_score = test_data['is_accurate'].mean()\n",
    "\n",
    "# Create the final table with all the required fields\n",
    "final_table = test_data[['utterance', 'category', 'intent', 'top_predict_intents', 'keywords_used', 'is_accurate']]\n",
    "\n",
    "# Display the final table and accuracy score\n",
    "display(final_table)\n",
    "print(f\"Accuracy Score: {accuracy_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate the accuracy rate for each intent\n",
    "intent_accuracy = test_data.groupby('intent')['is_accurate'].mean().sort_values(ascending=False)\n",
    "intent_accuracy_df = intent_accuracy.reset_index()\n",
    "intent_accuracy_df.columns = ['Intent', 'Accuracy Rate']\n",
    "intent_accuracy_df['Accuracy Rate'] = intent_accuracy_df['Accuracy Rate'].round(2)\n",
    "\n",
    "# Generate classification report for the model predictions\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame\n",
    "report_df = pd.DataFrame(report).transpose().reset_index()\n",
    "report_df.columns = ['Intent', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
    "report_df[['Precision', 'Recall', 'F1-Score']] = report_df[['Precision', 'Recall', 'F1-Score']].round(2)\n",
    "report_df['Support'] = report_df['Support']\n",
    "\n",
    "# Merge the accuracy rate DataFrame with the classification report DataFrame\n",
    "merged_df = pd.merge(intent_accuracy_df, report_df, on='Intent', how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "display(merged_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
